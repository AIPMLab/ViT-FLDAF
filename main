import time, copy, logging, numpy as np
from collections import Counter, defaultdict
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from torchvision import transforms
import timm
from sklearn.metrics import confusion_matrix

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---------- data loaders (keep your original paths) ----------
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])
Num_clients = 4
pathes = './RCNA_ICH/client_'
datasets = [ImageFolder(root=pathes+str(i), transform=transform) for i in range(Num_clients)]
train_datasets = [datasets[i] for i in range(Num_clients-1)]
test_dataset = datasets[Num_clients-1]
train_loaders = [DataLoader(ds, batch_size=32, shuffle=True, drop_last=True) for ds in train_datasets]
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

num_classes = len(datasets[0].classes)  # adjust if necessary

# ---------- model with learnable gamma as in original ----------
class CustomViT(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.model = timm.create_model('vit_small_patch16_224', pretrained=True)
        in_feat = self.model.head.in_features
        self.model.head = nn.Linear(in_feat, num_classes)
        self.gamma = nn.Parameter(torch.tensor(2.0, device=device), requires_grad=True)  # learnable gamma

    def forward(self, x):
        return self.model(x)

# ---------- imbalance/statistics utilities per 111.txt ----------
eps = 1e-6
def client_counts_from_dataset(dataset):
    cnt = Counter()
    for _, lbl in dataset:
        cnt[int(lbl)] += 1
    return dict(cnt)

def compute_client_ck(client_count, num_classes):
    N_k = sum(client_count.get(i,0) for i in range(num_classes))
    vals = []
    for i in range(num_classes):
        nki = client_count.get(i,0) + 1e-6
        vals.append((N_k - nki) / nki)
    return float(sum(vals) / num_classes), N_k

def compute_global_class_cfi(all_client_counts, all_Nk, num_classes):
    # c_{f,i} = (sum_k N_k - sum_k n_{k,i}) / sum_k n_{k,i}
    sum_Nk = sum(all_Nk)
    sum_nki = [0]*num_classes
    for kcnt in all_client_counts:
        for i in range(num_classes):
            sum_nki[i] += kcnt.get(i,0)
    cfi = []
    # class weight W_i optional
    for i in range(num_classes):
        denom = sum_nki[i] + eps
        cfi.append( (sum_Nk - sum_nki[i]) / denom )
    return cfi, sum_nki

# ---------- dynamic adaptive focal loss (per-sample, per-class scaling) ----------
def dafl_loss(logits, labels, cfi_list, gamma_param, ce_weight=1.0):
    """Implements: scaled focal per true-class: L = -(1 + c_{f,i}) * (1-pt)^gamma * log(pt)
       plus optional CE to stabilize as in paper.
    """
    probs = torch.softmax(logits, dim=1)
    batch = logits.shape[0]
    labels_long = labels.long()
    pt = probs[torch.arange(batch), labels_long].clamp(min=1e-9)
    cf_per_sample = torch.tensor([cfi_list[int(l)] for l in labels_long.cpu().numpy()], device=logits.device, dtype=pt.dtype)
    gamma = torch.clamp(gamma_param, min=0.0)
    focal = -(1.0 + cf_per_sample) * (1.0 - pt) ** gamma * torch.log(pt)
    focal_term = focal.mean()
    ce = nn.CrossEntropyLoss()(logits, labels_long)
    return focal_term + ce_weight * ce

# ---------- aggregation weight from client ck ----------
def aggregation_weights_from_ck(cks):
    # w_k = 1 / (1 + c_k) to avoid zero/neg issues, then normalize
    w = np.array([1.0 / (1.0 + max(0.0, ck) + eps) for ck in cks], dtype=float)
    w = w / (w.sum() + 1e-12)
    return w.tolist()

# ---------- local training (receives class-level cfi for this round) ----------
def local_train(model, train_loader, optimizer, scheduler, cfi_list, epochs=1):
    model.train()
    for e in range(epochs):
        total_loss = 0.0
        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            optimizer.zero_grad()
            logits = model(imgs)
            loss = dafl_loss(logits, labels, cfi_list, model.gamma)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        if scheduler is not None:
            scheduler.step()
        logging.info(f"Local epoch {e+1}/{epochs} loss {total_loss/len(train_loader):.4f}")
    return model

# ---------- server aggregation ----------
def federated_aggregate(server_model, client_models, agg_weights):
    server_sd = server_model.state_dict()
    # initialize server params as zeros
    new_state = {k: torch.zeros_like(v, dtype=torch.float32) for k,v in server_sd.items()}
    for w, cm in zip(agg_weights, client_models):
        csd = cm.state_dict()
        for k in new_state:
            new_state[k] += w * csd[k].to(torch.float32)
    server_model.load_state_dict(new_state)
    # broadcast back
    for cm in client_models:
        cm.load_state_dict(server_model.state_dict())
    return server_model, client_models

# ---------- orchestration ----------
def run_federated(rounds=50, local_epochs=1):
    # init models, optimizers, schedulers
    server_model = CustomViT(num_classes).to(device)
    client_models = [copy.deepcopy(server_model).to(device) for _ in range(len(train_loaders))]
    opts = [optim.Adam(cm.parameters(), lr=1e-4) for cm in client_models]
    scheds = [torch.optim.lr_scheduler.StepLR(opt, step_size=30, gamma=0.1) for opt in opts]

    # initial client counts (clients compute locally and send counts each round)
    client_counts = [client_counts_from_dataset(ds) for ds in train_datasets]
    all_Nk = [sum(cnt.values()) for cnt in client_counts]
    # initial compute (server)
    cks = []
    for cnt in client_counts:
        ck, Nk = compute_client_ck(cnt, num_classes)
        cks.append(ck)
    cfi_list, _ = compute_global_class_cfi(client_counts, all_Nk, num_classes)

    best_acc = 0.0
    for r in range(rounds):
        logging.info(f"--- Round {r+1}/{rounds} ---")
        # clients receive cfi_list and train locally
        for i in range(len(client_models)):
            client_models[i] = local_train(client_models[i], train_loaders[i], opts[i], scheds[i], cfi_list, epochs=local_epochs)

        # after local training, clients recompute and send counts (simulate)
        client_counts = [client_counts_from_dataset(ds) for ds in train_datasets]
        all_Nk = [sum(cnt.values()) for cnt in client_counts]
        # server computes client c_k and class-level cfi
        cks = []
        for cnt in client_counts:
            ck, Nk = compute_client_ck(cnt, num_classes)
            cks.append(ck)
        cfi_list, sum_nki = compute_global_class_cfi(client_counts, all_Nk, num_classes)

        # compute aggregation weights from cks
        agg_w = aggregation_weights_from_ck(cks)
        logging.info(f"Aggregation weights: {agg_w}")

        # aggregate
        server_model, client_models = federated_aggregate(server_model, client_models, agg_w)

        # eval
        acc, cm = evaluate(server_model, test_loader)
        if acc > best_acc:
            best_acc = acc
            torch.save(server_model.state_dict(), 'models/best_dafl.pth')
            logging.info(f"New best {best_acc:.2f}% saved.")

    logging.info("Federated training finished.")

def evaluate(model, loader):
    model.eval()
    correct = 0
    total = 0
    preds = []
    labels = []
    with torch.no_grad():
        for imgs, lbs in loader:
            imgs = imgs.to(device); lbs = lbs.to(device)
            logits = model(imgs)
            _, p = torch.max(logits, dim=1)
            correct += (p == lbs).sum().item()
            total += lbs.size(0)
            preds.extend(p.cpu().numpy()); labels.extend(lbs.cpu().numpy())
    acc = 100.0 * correct / total if total>0 else 0.0
    logging.info(f"Eval acc: {acc:.2f}%")
    logging.info("Confusion matrix:\n" + str(confusion_matrix(labels, preds)))
    return acc, confusion_matrix(labels, preds)

if __name__ == "__main__":
    run_federated(rounds=50, local_epochs=1)
