import numpy as np
import torch
import time
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
import torch.optim as optim
import timm
import copy
import logging
from sklearn.metrics import confusion_matrix

# 设置日志格式
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(levelname)s %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    handlers=[
                        logging.FileHandler("./FedVit_efficient_logs/dataset4_log", mode='w'),
                        logging.StreamHandler()
                    ])

# 确定使用的设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 数据增强与转换
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# 客户端数量和数据路径
Num_clients = 4
pathes = './data/RealSkin/client_'

# 数据加载
# 每个客户端对应一个数据集
client_datasets = [ImageFolder(root=pathes + str(idx), transform=transform) for idx in range(Num_clients)]
train_datasets = client_datasets[:-1]  # 前3个客户端的数据用于训练
test_dataset = client_datasets[-1]  # 第4个客户端的数据用于测试

# 数据加载器
train_loaders = [DataLoader(train_datasets[idx], batch_size=16, shuffle=True, drop_last=True)
                 for idx in range(Num_clients - 1)]
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, drop_last=True)

# 自定义ViT模型
class CustomViT(nn.Module):
    def __init__(self, num_classes):
        super(CustomViT, self).__init__()
        self.original_model = timm.create_model('vit_small_patch16_224', pretrained=True)
        self.original_model.head = nn.Linear(self.original_model.head.in_features, num_classes)

        self.mask_parameters = nn.Parameter(
            torch.ones(16, self.original_model.head.in_features, device=device),
            requires_grad=True)
        self.original_model.register_parameter('mask_parameters', self.mask_parameters)
        # self.BigMask = nn.Sequential(nn.Linear(self.original_model.head.in_features, self.original_model.head.in_features), nn.ReLU(),
        #                              nn.Softmax(dim=1)).to(device)
#set gama to be learnable and set the initial value as 2.0
        self.gamma = nn.Parameter(torch.tensor(2.0, device=device), requires_grad=True)
    def forward(self, x):
        # 提取特征直到最后一层之前
        # print(self.original_model.head.weight.shape, self.mask_parameters.shape)
        # masked_weight = self.original_model.head.weight * self.mask_parameters
        x = self.original_model.forward_features(x)
        x = self.original_model.norm(x)
        # x = torch.mul(x, self.mask_parameters)
        y = self.original_model.head(x)
        # x = self.original_model(x)
        # print('xxxx', x)

        # x = torch.(masked_weight, )
        # print(x.shape)
        # x_bigfake = self.BigMask(x)
        # x = torch.mul(x_bigfake, x)

        # if self.original_model.head.bias is not None:
        #     bias = self.original_model.blocks[7].mlp.fc2.bias
        #     Project = nn.Linear(768 ,4).to(device)
        #     bias  = Project(bias)
        # else:
        #     bias = 0

        # x = torch.addmm(bias, x, masked_weight.t())
        # print('X is:', x)
        return y

# 初始化全局模型和客户端模型
num_classes = 7
global_model = CustomViT(num_classes).to(device)
client_models = [copy.deepcopy(global_model) for _ in range(Num_clients - 1)]
for model in client_models:
    model.to(device)

# 定义全局不平衡参数
class GlobalImbalanceParameter(nn.Module):
    def __init__(self, num_classes):
        super(GlobalImbalanceParameter, self).__init__()
        self.params = nn.Parameter(torch.ones(num_classes, device=device), requires_grad=True)

global_imbalance_param = GlobalImbalanceParameter(num_classes)

# 定义客户端权重计算公式
def compute_client_weights(global_imbalance_param, train_loaders):
    """
    根据公式计算客户端权重：
    w_i = exp(-||G(c)_i - L(c)_i|| / tau)
    """
    tau = 1.0  # 平滑参数
    client_weights = []

    for idx, loader in enumerate(train_loaders):
        local_counts = torch.zeros(num_classes, device=device)
        for _, labels in loader:
            labels = labels.to(device)
            for cls in range(num_classes):
                local_counts[cls] += (labels == cls).sum()

        global_counts = global_imbalance_param.params
        distance = torch.norm(global_counts - local_counts, p=2)
        weight = torch.exp(-distance / tau)
        client_weights.append(weight)

    client_weights = torch.tensor(client_weights, device=device)
    client_weights = client_weights / client_weights.sum()
    return client_weights

# 定义自定义损失函数
def custom_loss(outputs, labels, model, global_imbalance_param, weight_decay=1e-5, sparsity_weight=5e-4):
    labels_one_hot = torch.nn.functional.one_hot(labels, num_classes=num_classes).float()
    probs = torch.nn.functional.softmax(outputs, dim=1)
    total_loss = 0

    for cls in range(num_classes):
        y_true = labels_one_hot[:, cls]
        y_pred = probs[:, cls]
        cf = global_imbalance_param.params[cls]
        gamma = model.gamma
        pt = torch.where(y_true == 1, y_pred, 1 - y_pred)
        focal_weight = (1 - pt) ** gamma
        focal_loss = -torch.abs(y_true - y_pred) ** gamma * (1 + cf) * torch.log(y_pred) * focal_weight
        total_loss += focal_loss.mean()

    cross_entropy_loss = nn.CrossEntropyLoss()(outputs, labels)
    total_loss += cross_entropy_loss
    total_loss += sparsity_weight * torch.norm(model.mask_parameters, p=1)
    return total_loss

# 联邦学习中的通信函数
def communication(server_model, models, client_weights):
    with torch.no_grad():
        for key in server_model.state_dict().keys():
            temp = torch.zeros_like(server_model.state_dict()[key], dtype=torch.float64)
            for client_idx, weight in enumerate(client_weights):
                temp += weight * models[client_idx].state_dict()[key]
            server_model.state_dict()[key].data.copy_(temp)
            for client_idx in range(len(models)):
                models[client_idx].state_dict()[key].data.copy_(server_model.state_dict()[key])
    return server_model, models

# 联邦训练流程
def federated_training(global_model, client_models, train_loaders, test_loader, global_imbalance_param):
    optimizers = [optim.Adam(model.parameters(), lr=1e-4) for model in client_models]
    schedulers = [optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) for optimizer in optimizers]
    start_time = time.time()
    best_accuracy = 0

    for epoch in range(50):
        # 计算客户端权重
        client_weights = compute_client_weights(global_imbalance_param, train_loaders)

        # 客户端本地训练
        for idx, model in enumerate(client_models):
            model.train()
            for inputs, labels in train_loaders[idx]:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizers[idx].zero_grad()
                outputs = model(inputs)
                loss = custom_loss(outputs, labels, model, global_imbalance_param)
                loss.backward()
                optimizers[idx].step()
            schedulers[idx].step()

        # 模型聚合
        global_model, client_models = communication(global_model, client_models, client_weights)

        # 在测试集上评估
        accuracy = evaluate(global_model, test_loader)
        best_accuracy = max(best_accuracy, accuracy)
        logging.info(f"Epoch {epoch+1}: Accuracy = {accuracy:.2f}%, Best = {best_accuracy:.2f}%")

    end_time = time.time()
    logging.info(f"Training completed in {end_time - start_time:.2f}s")
    return best_accuracy

# 模型评估函数
def evaluate(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
    return correct / total * 100

# 运行联邦训练
best_acc = federated_training(global_model, client_models, train_loaders, test_loader, global_imbalance_param)
torch.save(global_model.state_dict(), 'final_model.pth')
logging.info(f"Best accuracy: {best_acc:.2f}%")
